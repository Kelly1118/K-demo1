{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-04-14T07:39:15.395451300Z",
     "start_time": "2025-04-14T06:34:17.226433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "# 自定义数据集类\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length):\n",
    "        start_time = time.time()\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            self.texts = f.readlines()\n",
    "        end_time = time.time()\n",
    "        print(f\"数据加载耗时: {end_time - start_time} 秒\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': input_ids.clone()\n",
    "        }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 本地模型路径\n",
    "    local_model_path = r\"D:\\app\\Ollama\\deepseek-r1-1.5b\"\n",
    "    config_path = f\"{local_model_path}/config.json\"\n",
    "    tokenizer_path = f\"{local_model_path}/tokenizer.json\"\n",
    "\n",
    "    try:\n",
    "        # 检查配置文件是否存在\n",
    "        if not os.path.exists(config_path):\n",
    "            raise FileNotFoundError(f\"未找到 {config_path} 文件，请检查模型路径是否正确。\")\n",
    "        # 检查配置文件内容\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "            if 'model_type' not in config:\n",
    "                raise ValueError(f\"{config_path} 文件中缺少 'model_type' 字段，请检查文件内容。\")\n",
    "\n",
    "        # 检查分词器文件是否存在\n",
    "        if not os.path.exists(tokenizer_path):\n",
    "            raise FileNotFoundError(f\"未找到 {tokenizer_path} 文件，请检查模型路径是否正确。\")\n",
    "\n",
    "        # 加载预训练的tokenizer和模型\n",
    "        start_time = time.time()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            local_model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True  # 如果模型代码包含自定义部分，可能需要添加此参数\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        print(f\"模型加载耗时: {end_time - start_time} 秒\")\n",
    "\n",
    "        # 训练参数设置\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=2,\n",
    "            save_steps=10_000,\n",
    "            save_total_limit=2,\n",
    "            prediction_loss_only=True,\n",
    "            gradient_accumulation_steps=2,\n",
    "            logging_steps=100,  # 每 100 步记录一次日志\n",
    "            warmup_steps=500,  # 热身步数\n",
    "            weight_decay=0.01,  # 权重衰减\n",
    "            fp16=False,  # CPU不支持混合精度训练，设置为False\n",
    "            use_cpu=True  # 强制使用CPU\n",
    "        )\n",
    "\n",
    "        # 创建数据集\n",
    "        dataset = TextDataset('preprocessed_data.txt', tokenizer, max_length=512)\n",
    "\n",
    "        # 创建Trainer对象\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset,\n",
    "        )\n",
    "\n",
    "        print(\"开始训练前的准备工作...\")\n",
    "        start_time = time.time()\n",
    "        trainer.train()\n",
    "        end_time = time.time()\n",
    "        print(f\"训练总耗时: {end_time - start_time} 秒\")\n",
    "\n",
    "        # 保存训练好的模型\n",
    "        trainer.save_model('./fine_tuned_model')\n",
    "    except (FileNotFoundError, ValueError, OSError) as e:\n",
    "        print(f\"加载模型时出现错误: {e}，请检查本地模型路径 {local_model_path} 是否正确。\")"
   ],
   "id": "7905b60b3bffe796",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载耗时: 5.353346586227417 秒\n",
      "数据加载耗时: 0.0 秒\n",
      "开始训练前的准备工作...\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
